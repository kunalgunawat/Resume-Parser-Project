{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6f939411",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import logging\n",
    "import json\n",
    "import re\n",
    "\n",
    "# JSON formatting functions\n",
    "def convert_dataturks_to_spacy(dataturks_JSON_FilePath):\n",
    "    training_data = []\n",
    "    lines=[]\n",
    "    with open(dataturks_JSON_FilePath, 'r', encoding=\"utf8\") as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    for line in lines:\n",
    "        data = json.loads(line)\n",
    "        text = data['content'].replace(\"\\n\", \" \")\n",
    "        \n",
    "        entities = []\n",
    "        data_annotations = data['annotation']\n",
    "        if data_annotations is not None:\n",
    "            for annotation in data_annotations:\n",
    "                #only a single point in text annotation.\n",
    "                point = annotation['points'][0]\n",
    "                labels = annotation['label']\n",
    "                # handle both list of labels or a single label.\n",
    "                if not isinstance(labels, list):\n",
    "                    labels = [labels]\n",
    "\n",
    "                for label in labels:\n",
    "                    point_start = point['start']\n",
    "                    point_end = point['end']\n",
    "                    point_text = point['text']\n",
    "\n",
    "                    lstrip_diff = len(point_text) - len(point_text.lstrip())\n",
    "                    rstrip_diff = len(point_text) - len(point_text.rstrip())\n",
    "                    if lstrip_diff != 0:\n",
    "                        point_start = point_start + lstrip_diff\n",
    "                    if rstrip_diff != 0:\n",
    "                        point_end = point_end - rstrip_diff\n",
    "                    entities.append((point_start, point_end + 1 , label))\n",
    "        training_data.append((text, {\"entities\" : entities}))\n",
    "    return training_data\n",
    "\n",
    "def trim_entity_spans(data: list) -> list:\n",
    "    \"\"\"Removes leading and trailing white spaces from entity spans.\n",
    "\n",
    "    Args:\n",
    "        data (list): The data to be cleaned in spaCy JSON format.\n",
    "\n",
    "    Returns:\n",
    "        list: The cleaned data.\n",
    "    \"\"\"\n",
    "    invalid_span_tokens = re.compile(r'\\s')\n",
    "\n",
    "    cleaned_data = []\n",
    "    for text, annotations in data:\n",
    "        entities = annotations['entities']\n",
    "        valid_entities = []\n",
    "        for start, end, label in entities:\n",
    "            valid_start = start\n",
    "            valid_end = end\n",
    "            while valid_start < len(text) and invalid_span_tokens.match(\n",
    "                    text[valid_start]):\n",
    "                valid_start += 1\n",
    "            while valid_end > 1 and invalid_span_tokens.match(\n",
    "                    text[valid_end - 1]):\n",
    "                valid_end -= 1\n",
    "            valid_entities.append((valid_start, valid_end, label))\n",
    "        cleaned_data.append((text,valid_entities))\n",
    "    return cleaned_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "850642f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "220"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = trim_entity_spans(convert_dataturks_to_spacy(\"Entity Recognition in Resumes.json\"))\n",
    "\n",
    "# data1=convert_dataturks_to_spacy(\"Entity Recognition in Resumes.json\")\n",
    "# data2=preprocessing(data)\n",
    "# data=trim_entity_spans(data)\n",
    "    \n",
    "# # print(data[0])\n",
    "# # print()\n",
    "# # print()\n",
    "# print(data1[0])\n",
    "# print()\n",
    "# print()\n",
    "# print(data2[0])\n",
    "# print()\n",
    "# print(data1[0][0][316:349])\n",
    "# print(data2[0][0][372:405])\n",
    "len(data)\n",
    "# data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f9963bd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\"Abhijeet Manhas Email : abhijeetmanhas720@gmail.com github.com/abhijeetmanhas Mobile : +91-862-896-3924 twitter.com/astromanhas linkedin.com/abhimanhas Education \\\\x0fIndian Institute of Technology Mandi Mandi, India B.Tech in Computer Science Engineering; CGPA: 8.41 July 2018 { Present \\\\x0eRelevant coursework : Data Structures and Algorithms, Computing and Data Science, Data Science II, Data Science III, Applied Databases Practicum, Information and Database Systems, Linear Algebra. \\\\x0ePositions of Responsibility : Coordinator of Space Technology and Astronomy Cell, IIT Mandi for academic year 2019-20. Volunteered as Publicity head of AstraX 2020 (annual inter-college Astro-meet). Experience \\\\x0fGoogle Summer of Code Developer SunPy, OpenAstronomy Metadata Searches using Fido May 2020 - Present \\\\x0eWorking with SOAP and Web-Scrapers : Created methods to retrieve more details from XML responses received from querying Virtual Solar Observatory using Zeep Client. Build scrapers to support various archives for searching and fetching solar physics data in SunPy (python library for solar physics). \\\\x0eMaking Interface for querying DBMS Services : Added \\\\rexibility in Federated Internet Data Obtainer(Fido) to perform metadata only searches to web-services like Helio Event Knowledgebase(HEK) and JSOC database. Worked with astropy tables and hashing of response objects to ease data inspection and post-search \\\\x0cltering. Projects \\\\x0fHeyDoc App Ecosystem : Developed a Mobile(iOS and android) app which allows users to search doctors, book appointments, and get prescription noti\\\\x0ccations. It also includes DoctorDash - a web-app connected with HeyDoc through which doctors can register their clinics and manage bookings. Used \\\\rutter and Cloud Firestore. \\\\x0fWild\\\\x0crePy - Deputy Lead Developer : Co-author of an open-source python library for Geographic Information Systems (GIS) Data analysis for detecting wild\\\\x0cres. Worked on package's test suite, managing pip releases, and writing IO module for NASA EarthData \\\\x0cles. \\\\x0fDeep Generative Modelling for lensed Dark Matter : Used Generative Adversarial Neural Networks and Auto-Encoders to learn representation of dark matter images under strong gravitational lensing. Quanti\\\\x0ced the losses to compare DCGAN, AnoGAN and GANomaly for anomaly detection. \\\\x0fGymkhana Activities Calendar Web-App : Made a Django based event-scheduler application deployed on nignx that is used by institute's societies/clubs to book slots for their activities to avoid clashes. \\\\x0fGravis3D - Pseudo-Simulation of n-bodies under gravity : Allows to simulate Newtonian gravity model on real mass bodies on browser in 3D. Developed using astropy quantities and vpython. \\\\x0fBook Reviewer Android-App : An app through which book details like availability and reviews are fetched from various online stores by scanning its front page. Used OCR for text detection and jsoup for web-scraping. \\\\x0fAstro-Docker : A tool that can dockerize any python package in dev or stable mode using one command. Skills and Interests \\\\x0fLanguages : Python, Dart, C++, C, PHP, JavaScript. \\\\x0fTools : \\\\rutter, TensorFlow, Django, git, docker, VSCode, AstroPy, Matplotlib, Pandas, sklearn, keras, openCV. \\\\x0fGeneral : Backend Development, Deep Learning, Android Development, Astronomy, Data Science, Web scraping. Achievements, Awards and Extra-Curriculars \\\\x0fInter IIT Tech Meet 2019 : Won bronze medal among all IITs in a Data Science challenge organized by bitgrit Inc. involving NLP and Time Series Forecasting. \\\\x0fHon'ble Prime Minister Box Invitee, Republic Day Parade 2017 : Invited to PM Box to witness RD parade among 100 meritorious students pan India for extraordinary performance in CBSE AISSE, 2016. \\\\x0fIndian National Mathematics Olympiad 2017 : Received INMO merit certi\\\\x0ccate given to 50 high school students each year all across India issued by Homi Bhabha Centre For Science Education, TIFR. \\\\x0fWolfram One Award Winner, TopCoder Hackathon IIT Mandi : Made a web-app using NodeJS which shows the availability of medicine in local stores using the Google Maps API. Used MongoDB for dummy database. \\\\x0fEnigma , AstraX 2019 : Runners up in a 5-hour inter-college Computational Astrophysics Hackathon\",\n",
       " [[0, 15, 'Name'],\n",
       "  [313, 343, 'Skills'],\n",
       "  [359, 371, 'Skills'],\n",
       "  [3087, 3093, 'Skills'],\n",
       "  [3095, 3099, 'Skills'],\n",
       "  [3101, 3104, 'Skills'],\n",
       "  [3106, 3107, 'Skills'],\n",
       "  [3109, 3112, 'Skills'],\n",
       "  [3147, 3157, 'Skills'],\n",
       "  [3159, 3165, 'Skills'],\n",
       "  [3167, 3170, 'Skills'],\n",
       "  [3172, 3178, 'Skills'],\n",
       "  [3180, 3186, 'Skills'],\n",
       "  [3197, 3207, 'Skills'],\n",
       "  [3209, 3215, 'Skills'],\n",
       "  [3217, 3224, 'Skills'],\n",
       "  [3226, 3231, 'Skills'],\n",
       "  [3233, 3240, 'Skills'],\n",
       "  [3255, 3274, 'Skills'],\n",
       "  [3276, 3289, 'Skills'],\n",
       "  [3291, 3310, 'Skills'],\n",
       "  [3312, 3321, 'Skills'],\n",
       "  [3323, 3335, 'Skills'],\n",
       "  [3337, 3350, 'Skills']])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"Self_annot.json\", 'r', encoding=\"utf8\") as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "for line in lines:\n",
    "    sample = json.loads(line)\n",
    "    annot=sample[\"annotations\"]\n",
    "    text=annot[0][0]\n",
    "    entity=annot[0][1][\"entities\"]\n",
    "    data.append((text,entity))\n",
    "    \n",
    "len(data)\n",
    "data[230]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3008d694",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def clean_entities(training_data):\n",
    "    \n",
    "#     clean_data = []\n",
    "#     for text, annotation in training_data:\n",
    "        \n",
    "#         entities = annotation.get('entities')\n",
    "#         entities_copy = entities.copy()\n",
    "        \n",
    "#         # append entity only if it is longer than its overlapping entity\n",
    "#         i = 0\n",
    "#         for entity in entities_copy:\n",
    "#             j = 0\n",
    "#             for overlapping_entity in entities_copy:\n",
    "#                 # Skip self\n",
    "#                 if i != j:\n",
    "#                     e_start, e_end, oe_start, oe_end = entity[0], entity[1], overlapping_entity[0], overlapping_entity[1]\n",
    "#                     # Delete any entity that overlaps, keep if longer\n",
    "#                     if ((e_start >= oe_start and e_start <= oe_end) \\\n",
    "#                     or (e_end <= oe_end and e_end >= oe_start)) \\\n",
    "#                     and ((e_end - e_start) <= (oe_end - oe_start)):\n",
    "#                         entities.remove(entity)\n",
    "#                 j += 1\n",
    "#             i += 1\n",
    "#         clean_data.append((text, {'entities': entities}))\n",
    "                \n",
    "#     return clean_data\n",
    "\n",
    "# data = clean_entities(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ed84b6bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # print(data[5])\n",
    "\n",
    "# def function(data):\n",
    "#     processed=[]\n",
    "#     for sample in data:\n",
    "#         Merge=[]\n",
    "#         text=sample[0]\n",
    "#         entities=sample[1][\"entities\"]\n",
    "#         Start=entities[0][0]\n",
    "#         End=entities[0][1]\n",
    "#         Label=entities[0][2]\n",
    "#         temp_label=\"\"\n",
    "#         for entity in entities:\n",
    "#             if(Label==entity[2]):\n",
    "#                 Start=entity[0]\n",
    "#                 temp_label=Label\n",
    "#                 continue;\n",
    "#             else:\n",
    "#                 Merge.append((Start,End,temp_label))\n",
    "#                 Start=entity[0]\n",
    "#                 End=entity[1]\n",
    "#                 Label=entity[2]\n",
    "#                 temp_label=entity[2]\n",
    "                             \n",
    "         \n",
    "#         processed.append((text, {'entities': Merge}))\n",
    "         \n",
    "#     return processed\n",
    "# data=function(data)\n",
    "                \n",
    "    \n",
    "# data[50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1bf94e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install spacy_transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "97c421e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "252"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# !python -m spacy download en_core_web_lg\n",
    "# spacy.__version__\n",
    "len(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d37b5592",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 236/236 [00:02<00:00, 86.87it/s]\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.tokens import DocBin\n",
    "from tqdm import tqdm\n",
    "\n",
    "def get_spacy_doc(data):\n",
    "    nlp = spacy.blank(\"en\") # load a new spacy model\n",
    "    db = DocBin() # create a DocBin object\n",
    "\n",
    "    for text, annot in tqdm(data): # data in previous format\n",
    "        doc = nlp(text) # create doc object from text\n",
    "#         annot=annot[\"entities\"]\n",
    "        ents=[]\n",
    "        entity_indices=[]\n",
    "        for start, end, label in annot :\n",
    "            # add character indexes\n",
    "            skip_entity=False\n",
    "            for idx in range(start,end):\n",
    "                if idx in entity_indices:\n",
    "                    skip_entity=True\n",
    "                    break\n",
    "            if skip_entity==True:\n",
    "                continue\n",
    "            entity_indices=entity_indices+list(range(start,end))\n",
    "            \n",
    "            \n",
    "            try:\n",
    "                span = doc.char_span(start, end, label=label)\n",
    "            except:\n",
    "                continue\n",
    "            if span is None:\n",
    "#                 err_data=str([start,end])+\"   \"+str(text)+\"\\n\"\n",
    "#                 file.write(err_data)\n",
    "                continue\n",
    "            else:\n",
    "                ents.append(span)\n",
    "            \n",
    "        try:\n",
    "            doc.ents=ents\n",
    "            db.add(doc)\n",
    "        except:\n",
    "            pass\n",
    "    return db\n",
    "#     print(\"DATA CHANGE--------------------------------------------\")\n",
    "db=get_spacy_doc(data)\n",
    "db.to_disk(\"./train.spacy\") # save the docbin object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "83a01df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import train_test_split\n",
    "# train,test=train_test_split(data,test_size=0.3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "79bbd796",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 154/154 [00:03<00:00, 49.93it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 66/66 [00:01<00:00, 45.50it/s]\n"
     ]
    }
   ],
   "source": [
    "# # file=open(\"error.txt\",\"w\")\n",
    "# db=get_spacy_doc(train)\n",
    "# db.to_disk(\"train.spacy\")\n",
    "# db=get_spacy_doc(test)\n",
    "# db.to_disk(\"test.spacy\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4521a586",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] Auto-filled config with all values\n",
      "[+] Saved config\n",
      "config.cfg\n",
      "You can now add your data and train your pipeline:\n",
      "python -m spacy train config.cfg --paths.train ./train.spacy --paths.dev ./dev.spacy\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy init fill-config base_config.cfg config.cfg\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a159d642",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] Created output directory: output\n",
      "[i] Saving to output directory: output\n",
      "[i] Using CPU\n",
      "\u001b[1m\n",
      "=========================== Initializing pipeline ===========================\u001b[0m\n",
      "[+] Initialized pipeline\n",
      "\u001b[1m\n",
      "============================= Training pipeline =============================\u001b[0m\n",
      "[i] Pipeline: ['tok2vec', 'ner']\n",
      "[i] Initial learn rate: 0.001\n",
      "E    #       LOSS TOK2VEC  LOSS NER  ENTS_F  ENTS_P  ENTS_R  SCORE \n",
      "---  ------  ------------  --------  ------  ------  ------  ------\n",
      "[!] Aborting and saving the final best model. Encountered exception:\n",
      "MemoryError((133394, 768), dtype('float32'))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-11-22 15:00:38,299] [INFO] Set up nlp object from config\n",
      "[2022-11-22 15:00:38,307] [INFO] Pipeline: ['tok2vec', 'ner']\n",
      "[2022-11-22 15:00:38,309] [INFO] Created vocabulary\n",
      "[2022-11-22 15:00:40,247] [INFO] Added vectors: en_core_web_lg\n",
      "[2022-11-22 15:00:41,443] [INFO] Finished initializing nlp object\n",
      "[2022-11-22 15:01:03,516] [INFO] Initialized pipeline components: ['tok2vec', 'ner']\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Alok Kumar\\anaconda3\\lib\\runpy.py\", line 197, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"C:\\Users\\Alok Kumar\\anaconda3\\lib\\runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"C:\\Users\\Alok Kumar\\anaconda3\\lib\\site-packages\\spacy\\__main__.py\", line 4, in <module>\n",
      "    setup_cli()\n",
      "  File \"C:\\Users\\Alok Kumar\\anaconda3\\lib\\site-packages\\spacy\\cli\\_util.py\", line 71, in setup_cli\n",
      "    command(prog_name=COMMAND)\n",
      "  File \"C:\\Users\\Alok Kumar\\anaconda3\\lib\\site-packages\\click\\core.py\", line 1128, in __call__\n",
      "    return self.main(*args, **kwargs)\n",
      "  File \"C:\\Users\\Alok Kumar\\anaconda3\\lib\\site-packages\\typer\\core.py\", line 778, in main\n",
      "    return _main(\n",
      "  File \"C:\\Users\\Alok Kumar\\anaconda3\\lib\\site-packages\\typer\\core.py\", line 216, in _main\n",
      "    rv = self.invoke(ctx)\n",
      "  File \"C:\\Users\\Alok Kumar\\anaconda3\\lib\\site-packages\\click\\core.py\", line 1659, in invoke\n",
      "    return _process_result(sub_ctx.command.invoke(sub_ctx))\n",
      "  File \"C:\\Users\\Alok Kumar\\anaconda3\\lib\\site-packages\\click\\core.py\", line 1395, in invoke\n",
      "    return ctx.invoke(self.callback, **ctx.params)\n",
      "  File \"C:\\Users\\Alok Kumar\\anaconda3\\lib\\site-packages\\click\\core.py\", line 754, in invoke\n",
      "    return __callback(*args, **kwargs)\n",
      "  File \"C:\\Users\\Alok Kumar\\anaconda3\\lib\\site-packages\\typer\\main.py\", line 683, in wrapper\n",
      "    return callback(**use_params)  # type: ignore\n",
      "  File \"C:\\Users\\Alok Kumar\\anaconda3\\lib\\site-packages\\spacy\\cli\\train.py\", line 45, in train_cli\n",
      "    train(config_path, output_path, use_gpu=use_gpu, overrides=overrides)\n",
      "  File \"C:\\Users\\Alok Kumar\\anaconda3\\lib\\site-packages\\spacy\\cli\\train.py\", line 75, in train\n",
      "    train_nlp(nlp, output_path, use_gpu=use_gpu, stdout=sys.stdout, stderr=sys.stderr)\n",
      "  File \"C:\\Users\\Alok Kumar\\anaconda3\\lib\\site-packages\\spacy\\training\\loop.py\", line 122, in train\n",
      "    raise e\n",
      "  File \"C:\\Users\\Alok Kumar\\anaconda3\\lib\\site-packages\\spacy\\training\\loop.py\", line 105, in train\n",
      "    for batch, info, is_best_checkpoint in training_step_iterator:\n",
      "  File \"C:\\Users\\Alok Kumar\\anaconda3\\lib\\site-packages\\spacy\\training\\loop.py\", line 226, in train_while_improving\n",
      "    score, other_scores = evaluate()\n",
      "  File \"C:\\Users\\Alok Kumar\\anaconda3\\lib\\site-packages\\spacy\\training\\loop.py\", line 281, in evaluate\n",
      "    scores = nlp.evaluate(dev_corpus(nlp))\n",
      "  File \"C:\\Users\\Alok Kumar\\anaconda3\\lib\\site-packages\\spacy\\language.py\", line 1430, in evaluate\n",
      "    for eg, doc in zip(examples, docs):\n",
      "  File \"C:\\Users\\Alok Kumar\\anaconda3\\lib\\site-packages\\spacy\\language.py\", line 1589, in pipe\n",
      "    for doc in docs:\n",
      "  File \"C:\\Users\\Alok Kumar\\anaconda3\\lib\\site-packages\\spacy\\util.py\", line 1651, in _pipe\n",
      "    yield from proc.pipe(docs, **kwargs)\n",
      "  File \"spacy\\pipeline\\transition_parser.pyx\", line 233, in pipe\n",
      "  File \"C:\\Users\\Alok Kumar\\anaconda3\\lib\\site-packages\\spacy\\util.py\", line 1600, in minibatch\n",
      "    batch = list(itertools.islice(items, int(batch_size)))\n",
      "  File \"C:\\Users\\Alok Kumar\\anaconda3\\lib\\site-packages\\spacy\\util.py\", line 1651, in _pipe\n",
      "    yield from proc.pipe(docs, **kwargs)\n",
      "  File \"spacy\\pipeline\\trainable_pipe.pyx\", line 79, in pipe\n",
      "  File \"C:\\Users\\Alok Kumar\\anaconda3\\lib\\site-packages\\spacy\\util.py\", line 1670, in raise_error\n",
      "    raise e\n",
      "  File \"spacy\\pipeline\\trainable_pipe.pyx\", line 75, in spacy.pipeline.trainable_pipe.TrainablePipe.pipe\n",
      "  File \"C:\\Users\\Alok Kumar\\anaconda3\\lib\\site-packages\\spacy\\pipeline\\tok2vec.py\", line 125, in predict\n",
      "    tokvecs = self.model.predict(docs)\n",
      "  File \"C:\\Users\\Alok Kumar\\anaconda3\\lib\\site-packages\\thinc\\model.py\", line 315, in predict\n",
      "    return self._func(self, X, is_train=False)[0]\n",
      "  File \"C:\\Users\\Alok Kumar\\anaconda3\\lib\\site-packages\\thinc\\layers\\chain.py\", line 55, in forward\n",
      "    Y, inc_layer_grad = layer(X, is_train=is_train)\n",
      "  File \"C:\\Users\\Alok Kumar\\anaconda3\\lib\\site-packages\\thinc\\model.py\", line 291, in __call__\n",
      "    return self._func(self, X, is_train=is_train)\n",
      "  File \"C:\\Users\\Alok Kumar\\anaconda3\\lib\\site-packages\\thinc\\layers\\with_array.py\", line 38, in forward\n",
      "    return cast(Tuple[SeqT, Callable], _list_forward(model, Xseq, is_train))\n",
      "  File \"C:\\Users\\Alok Kumar\\anaconda3\\lib\\site-packages\\thinc\\layers\\with_array.py\", line 73, in _list_forward\n",
      "    Yf, get_dXf = layer(Xf, is_train)\n",
      "  File \"C:\\Users\\Alok Kumar\\anaconda3\\lib\\site-packages\\thinc\\model.py\", line 291, in __call__\n",
      "    return self._func(self, X, is_train=is_train)\n",
      "  File \"C:\\Users\\Alok Kumar\\anaconda3\\lib\\site-packages\\thinc\\layers\\chain.py\", line 55, in forward\n",
      "    Y, inc_layer_grad = layer(X, is_train=is_train)\n",
      "  File \"C:\\Users\\Alok Kumar\\anaconda3\\lib\\site-packages\\thinc\\model.py\", line 291, in __call__\n",
      "    return self._func(self, X, is_train=is_train)\n",
      "  File \"C:\\Users\\Alok Kumar\\anaconda3\\lib\\site-packages\\thinc\\layers\\residual.py\", line 41, in forward\n",
      "    Y, backprop_layer = model.layers[0](X, is_train)\n",
      "  File \"C:\\Users\\Alok Kumar\\anaconda3\\lib\\site-packages\\thinc\\model.py\", line 291, in __call__\n",
      "    return self._func(self, X, is_train=is_train)\n",
      "  File \"C:\\Users\\Alok Kumar\\anaconda3\\lib\\site-packages\\thinc\\layers\\chain.py\", line 55, in forward\n",
      "    Y, inc_layer_grad = layer(X, is_train=is_train)\n",
      "  File \"C:\\Users\\Alok Kumar\\anaconda3\\lib\\site-packages\\thinc\\model.py\", line 291, in __call__\n",
      "    return self._func(self, X, is_train=is_train)\n",
      "  File \"C:\\Users\\Alok Kumar\\anaconda3\\lib\\site-packages\\thinc\\layers\\chain.py\", line 55, in forward\n",
      "    Y, inc_layer_grad = layer(X, is_train=is_train)\n",
      "  File \"C:\\Users\\Alok Kumar\\anaconda3\\lib\\site-packages\\thinc\\model.py\", line 291, in __call__\n",
      "    return self._func(self, X, is_train=is_train)\n",
      "  File \"C:\\Users\\Alok Kumar\\anaconda3\\lib\\site-packages\\thinc\\layers\\chain.py\", line 55, in forward\n",
      "    Y, inc_layer_grad = layer(X, is_train=is_train)\n",
      "  File \"C:\\Users\\Alok Kumar\\anaconda3\\lib\\site-packages\\thinc\\model.py\", line 291, in __call__\n",
      "    return self._func(self, X, is_train=is_train)\n",
      "  File \"C:\\Users\\Alok Kumar\\anaconda3\\lib\\site-packages\\thinc\\layers\\maxout.py\", line 49, in forward\n",
      "    Y = model.ops.gemm(X, W, trans2=True)\n",
      "  File \"thinc\\backends\\numpy_ops.pyx\", line 101, in thinc.backends.numpy_ops.NumpyOps.gemm\n",
      "  File \"blis\\py.pyx\", line 79, in blis.py.gemm\n",
      "numpy.core._exceptions._ArrayMemoryError: Unable to allocate 391. MiB for an array with shape (133394, 768) and data type float32\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy train config.cfg --output ./output --paths.train ./train.spacy --paths.dev ./train.spacy\n",
    "# !python -m pip --version\n",
    "# !python -m pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "18b1c342",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install spacy==2.1.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "a749e162",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train_model(train_data):\n",
    "#     nlp = spacy.blank('en')\n",
    "#     if 'ner' not in nlp.pipe_names:\n",
    "# #         ner = nlp.create_pipe('ner')\n",
    "#         ner=nlp.add_pipe(\"ner\")\n",
    "    \n",
    "#     for _, annotation in train_data:\n",
    "#         for ent in annotation['entities']:\n",
    "#             ner.add_label(ent[2])\n",
    "            \n",
    "    \n",
    "#     other_pipes = [pipe for pipe in nlp.pipe_names if pipe != 'ner']\n",
    "#     with nlp.disable_pipes(*other_pipes):  # only train NER\n",
    "#         optimizer = nlp.begin_training()\n",
    "#         for itn in range(10):\n",
    "#             print(\"Statring iteration \" + str(itn))\n",
    "#             random.shuffle(train_data)\n",
    "#             losses = {}\n",
    "#             index = 0\n",
    "#             for text, annotations in train_data:\n",
    "#                 try:\n",
    "#                     nlp.update(\n",
    "#                         [text],  # batch of texts\n",
    "#                         [annotations],  # batch of annotations\n",
    "#                         drop=0.2,  # dropout - make it harder to memorise data\n",
    "#                         sgd=optimizer,  # callable to update weights\n",
    "#                         losses=losses)\n",
    "#                 except Exception as e:\n",
    "#                     pass\n",
    "# #                 \n",
    "#             print(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "404435c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m spacy download en_core_web_lg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0d309822",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# train_model(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "d298ac93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ###\n",
    "# nlp.to_disk('nlp_model')\n",
    "# nlp_model = spacy.load('nlp_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6337d2f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\"Anil Kumar Microsoft Azure (Basic Management)  Delhi, Delhi - Email me on Indeed: indeed.com/r/Anil-Kumar/96983a9dd7222ae5  Seeking a challenging career as an IT-Manager utilizes my analytical skills experience in design and development area of IT development & management. Long term objective is to achieve an IT consultant position.  STRENGTH:  Ability of solving problems independently, positive approach, quick learner and good performer both in team & independent environment, hardworking self-confident, sincere and deterministic.  WORK EXPERIENCE  Microsoft Azure (Basic Management)  FTP and TELNET -  2008 to 2008  • Design & implementation of Servers, Exchange Servers and Network according to the customer specific needs. • Designing and implementations of LAN /WAN, protocols used like TCP /IP, UDP, DHCP, HTTP, FTP and TELNET. • Providing remote support for maintaining Pentium, Pentium II, Pentium III, Servers on Windows NT, Windows 2000 Server, MS Windows Server 2003 R2 and MS Windows Server 2008 R2. • Software Office 97 Pro, Office 2000 pro. Office XP Pro, Office 2003, Microsoft Office 2007, Microsoft Office 2010, Microsoft Office 2013, Microsoft Office 2016 & Visual studio installation, MacAfee, Norton Antivirus, Escan Antivirus and other software & support. • Configuration of E-mail Clint software's that is (Outlook Express, Windows mail and Microsoft Outlook. • Installation and maintenance of Intel dual processor servers like HP, IBM, Dell Tower and Rack based server. • Managing Network Racks, LAN & WAN. • Installation and maintained of Backup media and devices with data backup and data restoration. • Maintenance and support of all type of desktops and Notebooks like HP, IBM, and HCL, Lenovo, Dell, Toshiba, Apple Mac book and All kind of Handheld Devices Etc. OPERATING SYSTEMS INSTALLED: • Microsoft Azure (Basic Management) • Microsoft Windows 2008 Server • Microsoft Windows 2003 Server • Microsoft Windows 2000 Server • Windows 8.1 Professional  https://www.indeed.com/r/Anil-Kumar/96983a9dd7222ae5?isid=rex-download&ikw=download-top&co=IN   • Windows 8 Professional • Windows 7 Professional • Microsoft Windows Vista • Microsoft Windows 2000 Prof. • Microsoft Windows XP Prof. • Microsoft Windows ME • Microsoft Windows 98 • Microsoft Windows 95  EDUCATION  MG University  Certification  OXFORD SOFTWARE INSTITUTE  SKILLS  DATA BACKUP (1 year), EXCHANGE (1 year), LAN (1 year), MAINTENANCE (1 year), SAP (1 year)  ADDITIONAL INFORMATION  SKILLS  Server Management, Basic Web content management by Joomla or WordPress, Desktop/Notebook management, IT consultancy, Data management, Software management, Network management (Active/Passive devices), Vendor development.  PROFESSIONAL SUUMARY:  13.7 years IT experience in implementation of LAN, WAN, MPLS, Server installation and maintenance, EPABX with VOIP function, Assembling of computers, maintenance and System Administration of Pentium pro, Pentium III, Pentium 4 and Servers, Software, Hardware and Network management at different organization.  1. TotipotentSC Scientific Product Pvt. Ltd. Gurgoan (Cesca Therapeutics Inc. US): - Working as Assistant Manager-IT from 10th September 2011 to present with all IT job responsibilities include IT management, vendor development, vendor management, solutions, IT asset purchasing, backup of all servers, SAP database server backup in NAS with Matrix and NEC Topaz EPABX management.  Spiceworks:- Spiceworks ticketing software management tool implementation for All systems tickets management.  Cyberoam CR50ia:- Implementation did with Secure VPN Tunnel from Udyog Vihar office to Fortis Stem Cell lab and implement user based security policy like TCP and IP traffic, remote user's secured login, Firewall and all other policies implementation.  TotipotentRX Cell Therapy Pvt. Ltd.:- Lab setup in Fortis Gurgaon with Airtel Lease Line 1 MB    with CCTV and Honeywell Biometric Access Pro 3000. Cyberoam CR35ing: - Implementation did with Secure VPN Tunnel from Fortis Stem cell Lab to Suncity Success Tower Office and implement user based security policy like TCP and IP traffic, remote user's secured login, Firewall and all other policies implementation.  2. Pal Business Systems: - Working as a Senior Engineer 24th March 2011 to 9th September 2011.  TotipotentSC Scientific Product Pvt. Ltd. Gurgoan: - Working as an IT-Consultant & SAP Coordinator for TotipotentSC to Upgrading the Complete IT setup on PAN India. Implementation of Domain Server with all the security policies, SAP Server and regular data backup, Thecus 16 TB NAS with automatic backup with secure manage IT Data. Implementation History: - 2 IBM X3320 Server, 1 is Domain Controller with users Dedicated Shared Folder as per data security point of view and 2nd is for SAP Server working as a SAP Coordinator daily basis SAP data backup via SQL Server 2008.  NAS:- Thecus 16 TB Disk space and 8 TB uses as a Data backup and rest 8 TB for mirroring, and taken the all server backup with all users data.  LCD Console: - Aten LCD Console 18.5 with 4:1 Dlink KVM Switch.  Anti Virus: - EScan Anti Virus Implementation has done in all users with Data Security Policy.  Mailing Software: - IQuinox Postmaster Enterprise implementation has done with the entire Local and remote user's configuration.  Leave for Studies. Leaves for Studies from 1st February 2010 to 23rd March 2011.  3. Dayal Computers: - Working as a Technical Head 9th October 2009 to 31 January 2010. Managing all the engineers and Project deployment client management, manage call escalation and design Corporate Solution to the customers.  4. INTERLINK Information Systems Pvt. Ltd: - Working As a senior customer support Executive Network & System Maintenance 18th February 2008 to October 2009.  THE MICRONUTRIENT INITIATIVE: - Working as an IT-Consultant for the Micronutrient Initiative with all the job responsibilities of an IT-Manager through Interlink Information Systems Pvt. Ltd. From 1st July 2008 to 20th February 2009 With Canada based Exchange server 2003, HP Prolaint ML150, Tandberg Data Ultrium backup device, 4 ISDN Lines and Cisco PIX Firewall 501 and with hand held devices like Blackberry, Remote Technical Support for Asia pacific, And Coordinate with different Vender's for different IT-Assets.  PROJECTS IN THE MICRONUTRIENT INITIATIVE: - 1. FIRE PROFF SAFE Purchased for MI- IT for keep the data cartridge  2. Upgrade Alcatel-Lucent OmniPCX Office system M cabinet to L cabinet with 24 extra ports with VOIP, Integrated CTI Server (PIMPhony PC telephony Software), Alcatel Integrated Voice Mail System, and Personal Assistant with 5 destinations.    INTERNATIONAL AIDS VACCINE INITIATIVE: - Working as a System-Cum-Server Administrator through Interlink Information Systems Pvt. Ltd. From 1st March 2008 to 30th June 2008 With IBM X Series 226 Xeon Server and IBM X Series 205 P-IV Server Network Printers, NAS and with hand held devices like Blackberry.  5. Sysware InfoTech Pvt. Ltd. Delhi: worked as a senior customer support Engineer Network & System Maintenance 23rd July 2003 to 10th February 2008. National Dairy Research Institute Karnal: Posted as A Network and System Maintenance Engineer (Resident) Through SIPL, Delhi at NDRI Karnal from 1st August 2005 to 16th August 2006 in 250 Desktops, 300 Printers (Lasers and Desk Jet, MFP, Scanners) and 250 Online and Offline UPS, s.  Vipul Infrastructure Developers Ltd: Posted as a Network and System Maintenance Engineer (Resident) Through SIPL, Delhi from 1st July 2005 to 30th July 2005 in 160 Desktops and 50 Notebooks with Manage Cisco L-2 and L-3 Devices.  Tecumseh Product India Pvt. Ltd. Hyderabad: Posted as A Network Engineer (Resident) through SIPL, Delhi, from 1st March 2005 to and 30th July 2005 rebuilt complete Network (range of D-Link and Cisco Active and Passive components) 200 Desktops and 50 Notebooks.  Tecumseh Products India Pvt. Ltd. Ballabhgarh: Posted as a Network and System Maintenance Engineer (Resident) Through SIPL, Delhi from 25th June 2004 to 27th February 2005 in 210 Desktops and 40 Notebooks with Cisco and D-Link Active and Passive Network Devices.  Network Accessories Installed:  • UTM's • Routers (1700, 3600 Series.) • Access points (D-link, Linksys) • Network Media [RJ 45, UTP Cat 3, 4, 5, 6, 7 • Media Converters • OFC Termination (With SC, ST Connectors) • Modems • Hubs • Switches (L-2) • Switches (L-3) • Vlans • CCTV DVR's Hikvision\",\n",
       " [(8004, 8009, 'Location'),\n",
       "  (7716, 7721, 'Location'),\n",
       "  (7502, 7507, 'Location'),\n",
       "  (7223, 7228, 'Location'),\n",
       "  (6985, 6990, 'Location'),\n",
       "  (2363, 2452, 'Skills'),\n",
       "  (1826, 1860, 'Designation'),\n",
       "  (555, 589, 'Designation'),\n",
       "  (82, 122, 'Email Address'),\n",
       "  (54, 59, 'Location'),\n",
       "  (47, 52, 'Location'),\n",
       "  (11, 45, 'Designation'),\n",
       "  (0, 10, 'Name')])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ####\n",
    "# doc = nlp_model(data[0][0])\n",
    "# for ent in doc.ents:\n",
    "#     print(f'{ent.label_.upper():{30}}- {ent.text}')\n",
    "    \n",
    "data[216]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "86e8d795",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !python -m spacy init fill-config base_config.cfg config.cfg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5f7cc2e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m spacy train config.cfg --output ./output --paths.train ./train.spacy --paths.dev ./train.spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e067da52",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp1 = spacy.load(r\"./output/model-best\") #load the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c779d6a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------------------------------------------\n",
      "Siddharth Choudhary -------->>>>>>> Name\n",
      "-----------------------------------------------------------------------------------------------------------------\n",
      "Microsoft Office Suite - Expert -------->>>>>>> Designation\n",
      "-----------------------------------------------------------------------------------------------------------------\n",
      "Hyderabad -------->>>>>>> Location\n",
      "-----------------------------------------------------------------------------------------------------------------\n",
      "indeed.com/r/Siddharth- Choudhary/19d56a964e37fa1a -------->>>>>>> Email Address\n",
      "-----------------------------------------------------------------------------------------------------------------\n",
      "Microsoft Office Suite - Expert -------->>>>>>> Designation\n",
      "-----------------------------------------------------------------------------------------------------------------\n",
      "AUDITING (Less than 1 year), CFA (Less than 1 year), DERIVATIVES (Less than 1 year), FINANCIAL ANALYST (Less than 1 year), FINANCIAL STATEMENT ANALYSIS (Less than 1 year)  ADDITIONAL INFORMATION  CORE COMPETENCIES  212, Shri Gayathri Elegant, Karkhana, Sec-bad. Portfolio management strategies Auditing (Statutory) Derivatives and Swaps Financial statement analysis Consolidation Ratio/Trend Analysis  COMPUTER SKILLS 2017- Chartered Level 1 - CFA (US)  https://www.indeed.com/r/Siddharth-Choudhary/19d56a964e37fa1a?isid=rex-download&ikw=download-top&co=IN https://www.indeed.com/r/Siddharth-Choudhary/19d56a964e37fa1a?isid=rex-download&ikw=download-top&co=IN   Present Financial Analyst Cleared -------->>>>>>> Skills\n"
     ]
    }
   ],
   "source": [
    "doc = nlp1(data[217][0]) # input sample text\n",
    "for ent in doc.ents:\n",
    "    print(\"-----------------------------------------------------------------------------------------------------------------\")\n",
    "    \n",
    "        \n",
    "    print(ent.text,\"-------->>>>>>>\",ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50317219",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
